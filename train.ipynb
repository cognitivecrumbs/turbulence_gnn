{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b8b9392f",
   "metadata": {},
   "source": [
    "# GNN Turbulence Model\n",
    "## Load in dataset\n",
    "### Dataset is in cartesian, convert to staggered points in "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8a9b196",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import numpy as np\n",
    "import torch\n",
    "from tqdm.notebook import tqdm\n",
    "\n",
    "\n",
    "# %matplotlib ipympl\n",
    "# import matplotlib as mpl\n",
    "# mpl.rcParams.update(mpl.rcParamsDefault)\n",
    "from cmap import Colormap\n",
    "from matplotlib import pyplot as plt\n",
    "### standard parameters\n",
    "plt.rcParams['figure.figsize'] = (6, 2)\n",
    "plt.rcParams['figure.dpi'] = 200\n",
    "plt.rcParams['animation.embed_limit'] = 2**128\n",
    "icefire_mpl = Colormap('icefire').to_matplotlib()\n",
    "try:\n",
    "    plt.colormaps.register(icefire_mpl, name='icefire')\n",
    "except:\n",
    "    pass\n",
    "plt.rcParams[\"image.cmap\"] = 'icefire'\n",
    "\n",
    "import matplotlib.animation as animation\n",
    "from IPython.display import HTML\n",
    "\n",
    "from mpl_toolkits.axes_grid1 import make_axes_locatable\n",
    "\n",
    "from scipy.spatial import Voronoi, Delaunay\n",
    "\n",
    "from models.models import *\n",
    "from src.mesh_util import *\n",
    "from src.data_util import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "03314ba9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# load data\n",
    "\n",
    "# filenames = {'baseline_64': 'kolmogorov_re_1000/eval_2048x2048_64x64.nc'}\n",
    "# filenames = {'baseline_64': 'kolmogorov_re_1000/long_eval_2048x2048_64x64.nc'}\n",
    "# filenames = {'baseline_64': 'kolmogorov_re_1000/eval_64x64_64x64.nc'}\n",
    "# filenames = {'baseline_64': 'kolmogorov_re_1000_fig1/baseline_64x64.nc'}\n",
    "# filenames = {'baseline_64': 'kolmogorov_re_1000_fig1/baseline_256x256.nc'}\n",
    "# filenames = {'baseline_64': 'kolmogorov_re_1000_fig1/baseline_2048x2048.nc'}\n",
    "# filenames = {'baseline_64': 'kolmogorov_re_4000/eval_128x128_128x128.nc'}\n",
    "# filenames = {'baseline_64': 'decaying/eval_1024x1024_64x64.nc'}\n",
    "filenames = {'baseline_64': 'decaying/eval_2048x2048_64x64.nc'}\n",
    "\n",
    "data = load_dataset(filenames,path='data/')\n",
    "data = data['baseline_64']\n",
    "dt = data.stable_time_step\n",
    "dt = (data.time[1]-data.time[0]).data\n",
    "\n",
    "x_range = (data.x.min().item(),data.x.max().item())\n",
    "y_range = (data.y.min().item(),data.y.max().item())\n",
    "\n",
    "x_range = (0,2*np.pi)\n",
    "y_range = (0,2*np.pi)\n",
    "\n",
    "print(x_range,y_range)\n",
    "print(dt)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38930eb3",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# setup mesh\n",
    "\n",
    "rows = 64\n",
    "cols = 64\n",
    "\n",
    "rows = 32\n",
    "cols = 32\n",
    "\n",
    "rows = 16\n",
    "cols = 16\n",
    "\n",
    "\n",
    "staggered_points = np.empty((rows,cols,2))\n",
    "\n",
    "dx = (x_range[1]-x_range[0])/(rows)\n",
    "dy = (y_range[1]-y_range[0])/(cols)\n",
    "staggered_points[:,:,1] = np.arange(y_range[0],y_range[1]-dy/10,dy)[:,np.newaxis]\n",
    "for i in range(rows):\n",
    "    point_row = np.arange(x_range[0],x_range[1]-6*dx/10,dx)\n",
    "    staggered_points[i,:,0] = point_row + i%2*(dx/2) # stagger\n",
    "    # staggered_points[i,:,0] = point_row \n",
    "\n",
    "staggered_points = staggered_points.reshape(-1,2)\n",
    "\n",
    "original_spacing = (data.x.max().values - data.x.min().values)/64\n",
    "mesh, connectivity, connectivity_periodic, edge_attr, edge_attr_periodic = delanay_mesh_constructor(staggered_points,periodic=True,bidirectional=True,periodic_limits=np.array([[x_range[0],x_range[1]],[y_range[0],y_range[1]]]))\n",
    "# mesh, connectivity, connectivity_periodic, edge_attr, edge_attr_periodic = cartesian_mesh_contructor(staggered_points,periodic=True,bidirectional=True)\n",
    "\n",
    "# fig = plt.figure(figsize=(16,6),dpi=300)\n",
    "fig = plt.figure()\n",
    "ax = fig.add_subplot(141)\n",
    "ax.plot(staggered_points[connectivity,0],staggered_points[connectivity,1],c='k')\n",
    "ax.set_aspect('equal')\n",
    "ax.set_title('Internal connections')\n",
    "ax.grid(True)\n",
    "ax = fig.add_subplot(142)\n",
    "ax.plot(staggered_points[connectivity_periodic,0],staggered_points[connectivity_periodic,1],alpha=0.2)\n",
    "ax.set_title('Periodic connections')\n",
    "ax.set_aspect('equal')\n",
    "ax.grid(True)\n",
    "\n",
    "# fig = plt.figure(figsize=(6,6),dpi=300)\n",
    "ax = fig.add_subplot(143)\n",
    "ax.scatter(staggered_points[:,0],staggered_points[:,1],s=1)\n",
    "ax.set_title('Mesh with additional points to generate periodic connections')\n",
    "ax.set_aspect('equal')\n",
    "# plt.show()\n",
    "\n",
    "# combine connectivities\n",
    "connectivity_combined = np.concatenate([connectivity,connectivity_periodic],axis=1)\n",
    "\n",
    "# combine relative position\n",
    "edge_attr_combined = np.concat([edge_attr,edge_attr_periodic],axis=0)\n",
    "\n",
    "# fig = plt.figure(figsize=(16,5),dpi=300)\n",
    "ax = fig.add_subplot(144)\n",
    "ax.scatter(edge_attr_combined[:,0],edge_attr_combined[:,1])\n",
    "ax.set_aspect('equal')\n",
    "ax.set_title('Connection Relative Postitions')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13d15356",
   "metadata": {},
   "source": [
    "## Interpolation of values\n",
    "### Original data is in cartesian system. Convert to polygon points from previously generated mesh"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02e5d367",
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.interpolate import RegularGridInterpolator\n",
    "\n",
    "rho = 1.0\n",
    "\n",
    "\n",
    "# plot figures\n",
    "timestep = 0\n",
    "sample_ind = 0\n",
    "# fig = plt.figure(figsize=(4,2),dpi=300)\n",
    "fig = plt.figure()\n",
    "ax = fig.add_subplot(141)\n",
    "ax.contourf(data.u[sample_ind,timestep])\n",
    "ax.set_aspect('equal')\n",
    "ax.set_title('u vel')\n",
    "ax = fig.add_subplot(142)\n",
    "ax.contourf(data.v[sample_ind,timestep])\n",
    "ax.set_aspect('equal')\n",
    "ax.set_title('v vel')\n",
    "# ax = fig.add_subplot(143)\n",
    "# ax.contourf(data.pressure[sample_ind,timestep])\n",
    "# ax.set_aspect('equal')\n",
    "# ax.set_title('pressure')\n",
    "ax = fig.add_subplot(144)\n",
    "ax.contourf(data.vorticity[sample_ind,timestep])\n",
    "ax.set_aspect('equal')\n",
    "ax.set_title('vorticity')\n",
    "\n",
    "\n",
    "plt.show()\n",
    "\n",
    "sample_inds = data.sample\n",
    "time_arr = data.time\n",
    "\n",
    "\n",
    "# periodicity of domain\n",
    "x = np.empty(len(data.x)+2)\n",
    "y = np.empty(len(data.y)+2)\n",
    "x[1:-1] = data.x\n",
    "y[1:-1] = data.y\n",
    "\n",
    "x[0] = data.x[0] - (data.x[1]-data.x[0])\n",
    "x[-1] = data.x[-1] + (data.x[-1]-data.x[-2])\n",
    "y[0] = data.y[0] - (data.y[1]-data.y[0])\n",
    "y[-1] = data.y[-1] + (data.y[-1]-data.y[-2])\n",
    "\n",
    "u = np.empty((*data.u.shape[:2],data.u.shape[2]+2,data.u.shape[3]+2))\n",
    "v = np.empty((*data.u.shape[:2],data.v.shape[2]+2,data.v.shape[3]+2))\n",
    "u[:,:,1:-1,1:-1] = data.u\n",
    "v[:,:,1:-1,1:-1] = data.v\n",
    "u[:,:,0] = u[:,:,-2]\n",
    "v[:,:,0] = v[:,:,-2]\n",
    "u[:,:,-1] = u[:,:,1]\n",
    "v[:,:,-1] = v[:,:,1]\n",
    "u[:,:,:,0] = u[:,:,:,-2]\n",
    "v[:,:,:,0] = v[:,:,:,-2]\n",
    "u[:,:,:,-1] = u[:,:,:,1]\n",
    "v[:,:,:,-1] = v[:,:,:,1]\n",
    "\n",
    "interp_u = RegularGridInterpolator([data.sample,data.time,x,y],u,bounds_error=False)\n",
    "interp_v = RegularGridInterpolator([data.sample,data.time,x,y],v,bounds_error=False)\n",
    "\n",
    "# reshape with samples and time steps\n",
    "# extend last dimension by 2 to include sample and time info\n",
    "staggered_points_full_set = np.tile(staggered_points,[len(data.sample),len(data.time),1,2])\n",
    "staggered_points_full_set[:,:,:,0] = data.sample.to_numpy()[:,np.newaxis,np.newaxis]\n",
    "staggered_points_full_set[:,:,:,1] = data.time.to_numpy()[np.newaxis,:,np.newaxis]\n",
    "flattened = staggered_points_full_set.reshape(-1,4) # [[sample, time, x, y], ...]\n",
    "\n",
    "staggered_points_u = interp_u(flattened) # [u, ...]\n",
    "staggered_points_v = interp_v(flattened)\n",
    "\n",
    "staggered_points_full_set_vel = np.stack([staggered_points_u.reshape(len(data.sample),len(data.time),rows,cols),staggered_points_v.reshape(len(data.sample),len(data.time),rows,cols)],axis=4)\n",
    "\n",
    "# fig = plt.figure(figsize=(1,2),dpi=300)\n",
    "fig = plt.figure()\n",
    "ax = fig.add_subplot(141)\n",
    "ax.tricontourf(staggered_points_full_set[0,0,:,2].flatten(),staggered_points_full_set[0,0,:,3].flatten(),staggered_points_full_set_vel[0,0,:,:,0].flatten())\n",
    "ax.set_aspect('equal')\n",
    "ax.set_title('u vel')\n",
    "ax = fig.add_subplot(142)\n",
    "ax.tricontourf(staggered_points_full_set[0,0,:,2].flatten(),staggered_points_full_set[0,0,:,3].flatten(),staggered_points_full_set_vel[0,0,:,:,1].flatten())\n",
    "ax.set_aspect('equal')\n",
    "ax.set_title('v vel')\n",
    "# ax = fig.add_subplot(143)\n",
    "# ax = fig.add_subplot(144)\n",
    "plt.show()\n",
    "\n",
    "# animate interpolated value evolution\n",
    "\n",
    "\n",
    "\n",
    "# import matplotlib.pyplot as plt\n",
    "# import matplotlib.animation as animation\n",
    "# import matplotlib.tri as tri\n",
    "# import numpy as np\n",
    "\n",
    "# 1. Initial plot\n",
    "fig = plt.figure()\n",
    "ax1 = fig.add_subplot(221)\n",
    "ax2 = fig.add_subplot(222)\n",
    "ax3 = fig.add_subplot(223)\n",
    "ax4 = fig.add_subplot(224)\n",
    "\n",
    "levels = np.linspace(-3, 3, 20)\n",
    "tcf1 = ax1.tricontourf(staggered_points_full_set[0,0,:,2].flatten(),\n",
    "                        staggered_points_full_set[0,0,:,3].flatten(),\n",
    "                        staggered_points_full_set_vel[0,0,:,:,0].flatten(), levels=levels)\n",
    "tcf2 = ax2.tricontourf(staggered_points_full_set[0,0,:,2].flatten(),\n",
    "                        staggered_points_full_set[0,0,:,3].flatten(),\n",
    "                        staggered_points_full_set_vel[0,0,:,:,1].flatten(), levels=levels)\n",
    "tcf3 = ax3.contourf(data.u[sample_ind,0].T, levels=levels)\n",
    "tcf4 = ax4.contourf(data.v[sample_ind,0].T, levels=levels)\n",
    "\n",
    "ax1.set_aspect('equal')\n",
    "ax2.set_aspect('equal')\n",
    "ax3.set_aspect('equal')\n",
    "ax4.set_aspect('equal')\n",
    "\n",
    "fig.colorbar(tcf1)\n",
    "fig.colorbar(tcf2)\n",
    "fig.colorbar(tcf3)\n",
    "fig.colorbar(tcf4)\n",
    "\n",
    "def animate(i):\n",
    "    global tcf1,tcf2,tcf3,tcf4\n",
    "    # Directly call remove on the TriContourSet object\n",
    "    if tcf1:\n",
    "        tcf1.remove()\n",
    "    if tcf2:\n",
    "        tcf2.remove()   \n",
    "    if tcf3:\n",
    "        tcf3.remove()\n",
    "    if tcf4:\n",
    "        tcf4.remove()\n",
    "    # Draw the new tricontourf\n",
    "    tcf1 = ax1.tricontourf(staggered_points_full_set[0,0,:,2].flatten(),\n",
    "                         staggered_points_full_set[0,0,:,3].flatten(),\n",
    "                         staggered_points_full_set_vel[0,i,:,:,0].flatten(), levels=levels)\n",
    "    tcf2 = ax2.tricontourf(staggered_points_full_set[0,0,:,2].flatten(),\n",
    "                            staggered_points_full_set[0,0,:,3].flatten(),\n",
    "                            staggered_points_full_set_vel[0,i,:,:,1].flatten(), levels=levels)\n",
    "    tcf3 = ax3.contourf(data.u[sample_ind,i].T, levels=levels)\n",
    "    tcf4 = ax4.contourf(data.v[sample_ind,i].T, levels=levels)\n",
    "    \n",
    "    ax1.set_title(f'Frame: {i}')\n",
    "    \n",
    "    # Since ContourSet is now a single artist, return it directly for blitting\n",
    "    return (tcf1,tcf2,tcf3,tcf4)\n",
    "\n",
    "# 3. Create the animation (blit=True is now supported)\n",
    "anim = animation.FuncAnimation(fig, animate, frames=staggered_points_full_set_vel.shape[1]-1, \n",
    "                               interval=40, blit=True)\n",
    "\n",
    "plt.show()\n",
    "HTML(anim.to_jshtml())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cbf8c9ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "ind_skip = 1\n",
    "unroll_inds = 10\n",
    "unroll_inds = 0\n",
    "device = 'mps'\n",
    "\n",
    "dt_skip = ind_skip*dt\n",
    "skip = staggered_points_full_set_vel[:,::ind_skip]\n",
    "train_test_split = 0.8\n",
    "\n",
    "train_data = skip[:int(skip.shape[0]*train_test_split)]\n",
    "test_data = skip[int(skip.shape[0]*train_test_split):]\n",
    "\n",
    "# torch.randperm()\n",
    "\n",
    "\n",
    "# set device\n",
    "e = torch.tensor(edge_attr_combined,dtype=torch.float32).to(device)\n",
    "train_input = torch.tensor(train_data[:,:-1-unroll_inds],dtype=torch.float32).to(device)\n",
    "train_target = torch.tensor(train_data[:,1:train_data.shape[1]-unroll_inds],dtype=torch.float32).to(device)\n",
    "if unroll_inds > 0:\n",
    "    train_unrolled_target = torch.tensor(train_data[:,1+unroll_inds:],dtype=torch.float32).to(device)\n",
    "train_target = (train_target - train_input)/dt_skip\n",
    "print(train_input.mean())\n",
    "print(train_input.std())\n",
    "print(train_target.mean())\n",
    "print(train_target.std())\n",
    "\n",
    "test_input = torch.tensor(test_data[:,:-1-unroll_inds],dtype=torch.float32).to(device)\n",
    "test_target = torch.tensor(test_data[:,1:test_data.shape[1]-unroll_inds],dtype=torch.float32).to(device)\n",
    "if unroll_inds > 0:\n",
    "    test_unrolled_target = torch.tensor(test_data[:,1+unroll_inds:],dtype=torch.float32).to(device)\n",
    "test_target = (test_target - test_input)/dt_skip\n",
    "# tmp = test_target.detach().clone()\n",
    "# tmp = tmp.reshape(tmp.shape[0],tmp.shape[1],rows*cols,2)\n",
    "\n",
    "ij = torch.tensor(connectivity_combined,dtype=torch.long).to(device)\n",
    "\n",
    "train_input = train_input.reshape(-1,rows*cols,2)\n",
    "test_input = test_input.reshape(-1,rows*cols,2)\n",
    "train_target = train_target.reshape(-1,rows*cols,2)\n",
    "test_target = test_target.reshape(-1,rows*cols,2)\n",
    "if unroll_inds > 0:\n",
    "    train_unrolled_target = train_unrolled_target.reshape(-1,rows*cols,2)\n",
    "    test_unrolled_target = test_unrolled_target.reshape(-1,rows*cols,2)\n",
    "\n",
    "print(train_input.shape)\n",
    "print(test_input.shape)\n",
    "print(train_target.shape)\n",
    "print(test_target.shape)\n",
    "print(ij.shape)\n",
    "print(e.shape)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71989097",
   "metadata": {},
   "outputs": [],
   "source": [
    "# energy_loss_lambda = 1e-8\n",
    "# energy_loss_lambda = 0\n",
    "\n",
    "\n",
    "# train model\n",
    "\n",
    "# construct model\n",
    "model = MeshGraphNet(npasses=4,\n",
    "                     ndim=16,\n",
    "                     node_fc_depth=2,\n",
    "                     edge_fc_depth=2,\n",
    "                     node_input_dim=2,\n",
    "                     edge_input_dim=2,\n",
    "                     activation=nn.SiLU(),\n",
    "                     dropout_rate=0.05).to(device)\n",
    "\n",
    "opt = torch.optim.Adam(model.parameters(),lr=1e-3)\n",
    "decay = torch.optim.lr_scheduler.ExponentialLR(opt,0.9999)\n",
    "# decay = torch.optim.lr_scheduler.ExponentialLR(opt,1.0)\n",
    "\n",
    "model.train()\n",
    "\n",
    "loss_hist = []\n",
    "test_loss_hist = []\n",
    "lr_hist = []\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4823cae1",
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_fn = torch.nn.MSELoss()\n",
    "if unroll_inds > 0:\n",
    "    def loss_fn(v,v_target,v_unroll,v_unroll_target,energy_loss_lambda=1e-12):\n",
    "        # standard mse\n",
    "        loss_mse = torch.nn.MSELoss()(v,v_target)\n",
    "        # additional energy loss\n",
    "        v_energy = (v_unroll**2).sum(-1).mean(-1)\n",
    "        v_target_energy = (v_unroll_target**2).sum(-1).mean(-1)\n",
    "        loss_energy = torch.nn.MSELoss()(v_energy,v_target_energy)\n",
    "        # loss_energy = (v_energy-v_target_energy)**2\n",
    "        return loss_mse + energy_loss_lambda*loss_energy\n",
    "\n",
    "\n",
    "batch_input = train_input[0:3]\n",
    "pred = model(batch_input,ij,e)\n",
    "if unroll_inds > 0:\n",
    "    batch_unroll = train_unrolled_target[0:3]\n",
    "    unrolling = pred.detach().clone()\n",
    "    for _ in range(unroll_inds):\n",
    "        unrolling = model(unrolling,ij,e)\n",
    "    loss = loss_fn(pred,train_target[0:3],unrolling,train_unrolled_target[0:3])\n",
    "else:\n",
    "    loss = loss_fn(pred,train_target[0:3])\n",
    "print(loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7165e45f",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "epochs = 100*6*8\n",
    "epochs = 10000\n",
    "batch_size = 64\n",
    "batch_size = 16\n",
    "test_interval = 10\n",
    "\n",
    "pbar = tqdm(range(epochs),position=0,leave=False)\n",
    "for i in pbar:\n",
    "    shuffled_inds = torch.randperm(train_input.shape[0])\n",
    "    shuffled_input = train_input[shuffled_inds]\n",
    "    shuffled_target = train_target[shuffled_inds]\n",
    "    train_loss_track = 0.0\n",
    "    if unroll_inds > 0:\n",
    "        shuffled_unroll_target = train_unrolled_target[shuffled_inds]\n",
    "    pbar2 = tqdm(range(int(train_input.shape[0]/batch_size)),position=1,leave=False)\n",
    "    for j in pbar2:\n",
    "    # for j in tqdm(range(int(train_input.shape[0]/batch_size)),position=1):\n",
    "        batch_input = shuffled_input[j*batch_size:(j+1)*batch_size]\n",
    "        batch_target = shuffled_target[j*batch_size:(j+1)*batch_size]\n",
    "        model.zero_grad()\n",
    "        pred = model(batch_input,ij,e)\n",
    "        if unroll_inds > 0:\n",
    "            batch_unroll_target = shuffled_unroll_target[j*batch_size:(j+1)*batch_size]\n",
    "            pred_unroll = pred.detach().clone()\n",
    "            for k in range(unroll_inds):\n",
    "                pred_unroll = model(pred_unroll,ij,e)\n",
    "            loss = loss_fn(pred,batch_target,pred_unroll,batch_unroll_target)\n",
    "        else:\n",
    "            loss = loss_fn(pred,batch_target)\n",
    "        \n",
    "        train_loss_track += loss.item()*batch_input.shape[0]/train_input.shape[0]\n",
    "        pbar2.set_postfix({'Train Loss': loss.item()})\n",
    "\n",
    "        # loss = loss_fn(pred,batch_target)\n",
    "    \n",
    "        loss.backward()\n",
    "        opt.step()\n",
    "    decay.step()\n",
    "\n",
    "    # tqdm.write(f\"Step {i}: Loss {loss.item()}\")\n",
    "    loss_hist.append(train_loss_track)\n",
    "    lr_hist.append(opt.param_groups[0]['lr'])\n",
    "\n",
    "    if i%test_interval == 0:\n",
    "        model.eval()\n",
    "        rand_sample = torch.randperm(test_input.shape[0])\n",
    "        test_loss_track = 0.0\n",
    "        pbar3 = tqdm(range(int(test_input.shape[0]/batch_size)),position=2,leave=False)\n",
    "        for j in pbar3:\n",
    "        # for j in tqdm(range(int(test_input.shape[0]/batch_size)),position=1):\n",
    "            batch_input = test_input[rand_sample][j*batch_size:(j+1)*batch_size]\n",
    "            batch_target = test_target[rand_sample][j*batch_size:(j+1)*batch_size]\n",
    "            pred_test = model(batch_input,ij,e)\n",
    "            if unroll_inds > 0:\n",
    "                batch_unroll_target = test_unrolled_target[rand_sample][j*batch_size:(j+1)*batch_size]\n",
    "                pred_unroll = pred_test.detach().clone()\n",
    "                for k in range(unroll_inds):\n",
    "                    pred_unroll = model(pred_unroll,ij,e)\n",
    "                test_loss = loss_fn(pred_test,batch_target,pred_unroll,batch_unroll_target)\n",
    "            else:\n",
    "                test_loss = loss_fn(pred_test,batch_target)\n",
    "\n",
    "            # test_loss = loss_fn(pred_test,batch_target)\n",
    "        \n",
    "            test_loss_track += test_loss.item()*batch_input.shape[0]/test_input.shape[0]\n",
    "            pbar3.set_postfix({'Test Loss': test_loss.item()})\n",
    "\n",
    "\n",
    "        test_loss_hist.append(test_loss_track)\n",
    "        model.train()\n",
    "    pbar.set_postfix({'Step': i, 'Train Loss': train_loss_track, 'Test Loss': test_loss_track,'LR': opt.param_groups[0]['lr']})\n",
    "\n",
    "# # plot loss\n",
    "# plt.figure()\n",
    "# plt.semilogy(loss)\n",
    "# plt.xlabel('Training Step')\n",
    "# plt.ylabel('MSE Loss')\n",
    "# plt.title('Training Loss Curve')\n",
    "# plt.grid()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d17f823",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "fig = plt.figure(figsize=(16,5),dpi=300)\n",
    "ax = fig.add_subplot(131)\n",
    "ax.scatter(edge_attr_combined[:,0],edge_attr_combined[:,1])\n",
    "ax.set_aspect('equal')\n",
    "ax.set_title('Connection Relative Postitions')\n",
    "\n",
    "# fig = plt.figure(figsize=(12,5))\n",
    "ax = fig.add_subplot(132)\n",
    "ax.plot(loss_hist,label='Train Loss')\n",
    "ax.plot(range(0,len(test_loss_hist)*test_interval,test_interval),test_loss_hist,label='Test Loss',c='orange')\n",
    "ax.legend()\n",
    "ax.set_yscale('log')\n",
    "ax.set_xlabel('Epoch')\n",
    "ax.set_ylabel('MSE Loss')\n",
    "ax.set_title('Training Loss')\n",
    "ax.grid(True)\n",
    "\n",
    "ax = fig.add_subplot(133)\n",
    "ax.plot(lr_hist,loss_hist, label='Train Loss', c='blue')\n",
    "ax.plot(lr_hist[::test_interval],test_loss_hist,label='Test Loss', c='orange')\n",
    "ax.set_xscale('log')\n",
    "ax.set_yscale('log')\n",
    "\n",
    "ax.set_xlabel('Learning Rate')\n",
    "ax.set_ylabel('MSE Loss')\n",
    "ax.set_title('Training Loss vs Learning Rate')\n",
    "ax.grid(True)\n",
    "\n",
    "fig.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3f4ee12",
   "metadata": {},
   "outputs": [],
   "source": [
    "# test stability\n",
    "inds = 0\n",
    "unroll = 20\n",
    "test_unroll_data = torch.Tensor(test_data[inds,:-1]).to(device)\n",
    "test_unroll_target = torch.Tensor(test_data[inds,1:]).to(device)\n",
    "\n",
    "# print(test_unroll_data.shape)\n",
    "# print(test_unroll_target.shape)\n",
    "\n",
    "test_unroll_data = test_unroll_data.reshape(-1,rows*cols,2)\n",
    "test_unroll_target = test_unroll_target.reshape(-1,rows*cols,2)\n",
    "\n",
    "velocity = test_unroll_data[0].unsqueeze(0)\n",
    "model_unroll = torch.empty_like(test_unroll_data).to(device)\n",
    "# tmp_unroll = torch.empty_like(test_unroll_data).to(device)\n",
    "\n",
    "\n",
    "# tmp_unroll[0] = test_unroll_data[0] + tmp[inds,0]*dt_skip\n",
    "\n",
    "model.eval()\n",
    "# print(test_unroll_data.shape)\n",
    "# print(model_unroll.shape)\n",
    "for i in range(test_unroll_data.shape[0]):\n",
    "    # velocity = model(velocity,ij,e)\n",
    "\n",
    "    dudt = model(velocity,ij,e)\n",
    "    velocity = dudt*dt_skip + velocity\n",
    "    model_unroll[i] = velocity[0].detach()\n",
    "\n",
    "    # if i > 0:\n",
    "    #     tmp_unroll[i] = tmp_unroll[i-1] + tmp[inds,i]*dt_skip \n",
    "\n",
    "\n",
    "# tmp_unroll = torch.cumsum(tmp[0],dim=0)*dt_skip + test_unroll_data[0]\n",
    "# wtf = torch.cumsum(tmp[0],dim=0)*dt_skip + test_unroll_data[0]\n",
    "\n",
    "# compare\n",
    "model_unroll = model_unroll.cpu().numpy()\n",
    "model_unroll = np.nan_to_num(model_unroll)\n",
    "# tmp_unroll = tmp_unroll.cpu().numpy()\n",
    "test_unroll_target = test_unroll_target.cpu().numpy()\n",
    "nplots = 10\n",
    "# time_inds = (np.linspace(0,test_data.shape[1]-2,nplots).astype(int))\n",
    "time_inds = (np.linspace(0,test_data.shape[1]/8,nplots).astype(int))\n",
    "\n",
    "# print(staggered_points_full_set_vel.shape)\n",
    "# print(test_unroll_target.shape)\n",
    "# print(model_unroll.shape)\n",
    "# print(tmp_unroll)\n",
    "# print(tmp.shape)\n",
    "\n",
    "\n",
    "fig = plt.figure(figsize=(3*nplots,5),dpi=300)\n",
    "for i,time_ind in enumerate(time_inds):\n",
    "    # i = int(i)\n",
    "    ax = fig.add_subplot(2,nplots,i+1)\n",
    "    px = ax.tricontourf(staggered_points[:,0],staggered_points[:,1],test_unroll_target[time_ind,:,0])\n",
    "    # ax.tricontourf(staggered_points[:,0],staggered_points[:,1],test_data[inds,i,:,0])\n",
    "    # ax.tricontourf(staggered_points[:,0],staggered_points[:,1],train_data[inds,i,:,0])\n",
    "    # ax.tricontourf(staggered_points[:,0],staggered_points[:,1],staggered_points_full_set_vel[inds,i*10,:,:,0].flatten())\n",
    "\n",
    "    ax.set_aspect('equal')\n",
    "    ax.set_title('Reference Data (%i,%2.4fs)'%(time_ind,time_ind*dt_skip))\n",
    "    fig.colorbar(px)\n",
    "\n",
    "    ax = fig.add_subplot(2,nplots,i+nplots+1)\n",
    "    px = ax.tricontourf(staggered_points[:,0],staggered_points[:,1],model_unroll[time_ind,:,0])\n",
    "    # px = ax.tricontourf(staggered_points[:,0],staggered_points[:,1],tmp_unroll[time_ind,:,0])\n",
    "    ax.set_aspect('equal')\n",
    "    ax.set_title('Predicted Data')\n",
    "    fig.colorbar(px)\n",
    "\n",
    "fig = plt.figure(figsize=(3*nplots,5),dpi=300)\n",
    "for i,time_ind in enumerate(time_inds):\n",
    "    # i = int(i)\n",
    "    ax = fig.add_subplot(2,nplots,i+1)\n",
    "    # ax.tricontourf(staggered_points[:,0],staggered_points[:,1],test_unroll_target[time_ind,:,1])\n",
    "    px = ax.tricontourf(staggered_points[:,0],staggered_points[:,1],test_unroll_target[time_ind,:,1])\n",
    "    # ax.tricontourf(staggered_points[:,0],staggered_points[:,1],test_data[inds,i,:,0])\n",
    "    # ax.tricontourf(staggered_points[:,0],staggered_points[:,1],train_data[inds,i,:,0])\n",
    "    # ax.tricontourf(staggered_points[:,0],staggered_points[:,1],staggered_points_full_set_vel[inds,i*10,:,:,0].flatten())\n",
    "    fig.colorbar(px)\n",
    "\n",
    "    ax.set_aspect('equal')\n",
    "    ax.set_title('Reference Data (%i,%2.4fs)'%(time_ind,time_ind*dt_skip))\n",
    "\n",
    "    ax = fig.add_subplot(2,nplots,i+nplots+1)\n",
    "    px = ax.tricontourf(staggered_points[:,0],staggered_points[:,1],model_unroll[time_ind,:,1])\n",
    "    ax.set_aspect('equal')\n",
    "    ax.set_title('Predicted Data')\n",
    "    fig.colorbar(px)\n",
    "\n",
    "plt.show()\n",
    "# print(test_data[inds,i,:,0])\n",
    "\n",
    "\n",
    "fig = plt.figure()\n",
    "ax1 = fig.add_subplot(221)\n",
    "ax2 = fig.add_subplot(222)\n",
    "ax3 = fig.add_subplot(223)\n",
    "ax4 = fig.add_subplot(224)\n",
    "\n",
    "levels = np.linspace(-3, 3, 20)\n",
    "tcf1 = ax1.tricontourf(staggered_points[:,0].flatten(),\n",
    "                        staggered_points[:,1].flatten(),\n",
    "                        test_unroll_target[0,:,0].flatten(), levels=levels)\n",
    "tcf2 = ax2.tricontourf(staggered_points[:,0].flatten(),\n",
    "                        staggered_points[:,1].flatten(),\n",
    "                        test_unroll_target[0,:,1].flatten(), levels=levels)\n",
    "tcf3 = ax3.tricontourf(staggered_points[:,0].flatten(),\n",
    "                        staggered_points[:,1].flatten(),\n",
    "                        model_unroll[0,:,0].flatten(), levels=levels)\n",
    "tcf4 = ax4.tricontourf(staggered_points[:,0].flatten(),\n",
    "                        staggered_points[:,1].flatten(),\n",
    "                        model_unroll[0,:,1].flatten(), levels=levels)\n",
    "\n",
    "\n",
    "\n",
    "ax1.set_aspect('equal')\n",
    "ax2.set_aspect('equal')\n",
    "ax3.set_aspect('equal')\n",
    "ax4.set_aspect('equal')\n",
    "\n",
    "fig.colorbar(tcf1)\n",
    "fig.colorbar(tcf2)\n",
    "fig.colorbar(tcf3)\n",
    "fig.colorbar(tcf4)\n",
    "\n",
    "def animate(i):\n",
    "    global tcf1,tcf2,tcf3,tcf4\n",
    "    # Directly call remove on the TriContourSet object\n",
    "    if tcf1:\n",
    "        tcf1.remove()\n",
    "    if tcf2:\n",
    "        tcf2.remove()   \n",
    "    if tcf3:\n",
    "        tcf3.remove()\n",
    "    if tcf4:\n",
    "        tcf4.remove()\n",
    "    # Draw the new tricontourf\n",
    "    tcf1 = ax1.tricontourf(staggered_points[:,0].flatten(),\n",
    "                            staggered_points[:,1].flatten(),\n",
    "                            test_unroll_target[i,:,0].flatten(), levels=levels)\n",
    "    tcf2 = ax2.tricontourf(staggered_points[:,0].flatten(),\n",
    "                            staggered_points[:,1].flatten(),\n",
    "                            test_unroll_target[i,:,1].flatten(), levels=levels)\n",
    "    tcf3 = ax3.tricontourf(staggered_points[:,0].flatten(),\n",
    "                            staggered_points[:,1].flatten(),\n",
    "                            model_unroll[i,:,0].flatten(), levels=levels)\n",
    "    tcf4 = ax4.tricontourf(staggered_points[:,0].flatten(),\n",
    "                            staggered_points[:,1].flatten(),\n",
    "                            model_unroll[i,:,1].flatten(), levels=levels)\n",
    "    \n",
    "    ax1.set_title(f'Frame: {i}')\n",
    "    \n",
    "    # Since ContourSet is now a single artist, return it directly for blitting\n",
    "    return (tcf1,tcf2,tcf3,tcf4)\n",
    "\n",
    "# 3. Create the animation (blit=True is now supported)\n",
    "anim = animation.FuncAnimation(fig, animate, frames=model_unroll.shape[0]-1, \n",
    "                               interval=40, blit=True)\n",
    "\n",
    "plt.show()\n",
    "HTML(anim.to_jshtml())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e148cdc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# calculate energy\n",
    "target_energy = (test_unroll_target ** 2).sum(-1).mean(-1)\n",
    "model_energy = (model_unroll ** 2).sum(-1).mean(-1)\n",
    "\n",
    "fig = plt.figure()\n",
    "ax1 = fig.add_subplot(111)\n",
    "ax2 = ax1.twinx()\n",
    "\n",
    "ax1.plot(target_energy,c='tab:blue',label='Target Energy')\n",
    "ax2.plot(model_energy,c='tab:orange',label='Predicted Energy')\n",
    "ax1.legend()\n",
    "# ax1.set_xlim([0,20])\n",
    "# ax2.set_xlim([0,20])\n",
    "ax1.set_ylim([0,2])\n",
    "ax2.set_ylim([0,2])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "29730feb",
   "metadata": {},
   "source": [
    "### Not sure what density is\n",
    "\n",
    "### Calculate from navier stokes equations (incompressible) with non dimensionalized length and time scales with reynolds number \n",
    "\n",
    "$\\frac{\\partial u_i}{\\partial t} = - \\frac{d p}{\\rho d x_i} - u_j \\frac{\\partial u_i}{\\partial x_j} + \\frac{1}{Re} \\frac{\\partial^2 u_j}{\\partial x_i \\partial x_j}$\n",
    "\n",
    "\n",
    "$\\frac{du_i}{dx_i} = 0$\n",
    "\n",
    "### Filtered Navier stokes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5731388f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# c\n",
    "re = 1000\n",
    "dt = data.stable_time_step\n",
    "\n",
    "dudx1 = data.u.differentiate('x')\n",
    "dvdy1 = data.v.differentiate('y')\n",
    "\n",
    "res = dudx1+dvdy1\n",
    "print('wtf continutity diff')\n",
    "print(np.sqrt(res[:,-1]**2).mean().values)\n",
    "\n",
    "print(data.u[:,0].mean().to_numpy())\n",
    "\n",
    "\n",
    "u = data.u.to_numpy()\n",
    "v = data.v.to_numpy()\n",
    "p = data.pressure.to_numpy()\n",
    "\n",
    "# u = u[:3,:3,:9,:9]\n",
    "# v = v[:3,:3,:9,:9]\n",
    "# p = p[:3,:3,:9,:9]\n",
    "\n",
    "\n",
    "dx = data.x[1] - data.x[0]\n",
    "dy = data.y[1] - data.y[0]\n",
    "\n",
    "dx = dx.values\n",
    "dy = dy.values\n",
    "\n",
    "# centered difference derivitive calc\n",
    "dudx = (np.roll(u,-1,axis=-2) - np.roll(u,1,axis=-2))/dx/2\n",
    "dudy = (np.roll(u,-1,axis=-1) - np.roll(u,1,axis=-1))/dy/2\n",
    "\n",
    "dvdx = (np.roll(v,-1,axis=-2) - np.roll(v,1,axis=-2))/dx/2\n",
    "dvdy = (np.roll(v,-1,axis=-1) - np.roll(v,1,axis=-1))/dy/2\n",
    "\n",
    "continuitiy_residual = dudx+dvdy\n",
    "\n",
    "# continuitiy_residual1 = 0.5*(np.roll(u,-1,axis=-2))\n",
    "\n",
    "print(np.sqrt(continuitiy_residual[:,-1]**2).mean())\n",
    "print(continuitiy_residual[0,0,3:-3,3:-3])\n",
    "\n",
    "# d2udxdy = np.gradient(dudx,axis=-1)/dy\n",
    "# check = np.gradient(dvdy,axis=-2)/dx\n",
    "\n",
    "# d2udxdy = (np.roll(dudx,-1,axis=-1)-np.roll(dudx,1,axis=-1))/dy/2\n",
    "d2udxdx = (np.roll(u,-1,axis=-2) - 2*u + np.roll(u,1,axis=-2))/dx**2\n",
    "d2udxdy = (np.roll(u,[-1,-1],axis=[-1,-2]) - np.roll(u,[-1,1],axis=[-1,-2]) - np.roll(u,[1,-1],axis=[-1,-2]) + np.roll(u,[1,1],axis=[-1,-2]))/dx/dy/4\n",
    "\n",
    "d2vdydy = (np.roll(v,-1,axis=-1) - 2*v + np.roll(v,1,axis=-1))/dy**2\n",
    "d2vdxdy = (np.roll(v,[-1,-1],axis=[-1,-2]) - np.roll(v,[-1,1],axis=[-1,-2]) - np.roll(v,[1,-1],axis=[-1,-2]) + np.roll(v,[1,1],axis=[-1,-2]))/dx/dy/4\n",
    "\n",
    "\n",
    "advection_1 = u * dudx + v * dudy \n",
    "advection_2 = u * dvdx + v * dvdy \n",
    "\n",
    "visc_1 = (d2udxdx+d2vdxdy)/re\n",
    "visc_2 = (d2udxdy+d2vdydy)/re\n",
    "\n",
    "dudt = (np.roll(u,-1,axis=1) - np.roll(u,1,axis=1))/dt/2\n",
    "dvdt = (np.roll(v,-1,axis=1) - np.roll(v,1,axis=1))/dt/2\n",
    "\n",
    "dpdx_invrho = -(dudt+advection_1-visc_1)\n",
    "dpdy_invrho = -(dvdt+advection_2-visc_2)\n",
    "\n",
    "# print(dpdx_invrho[0,1])\n",
    "\n",
    "bern_dpdx = (np.roll(p,-1,axis=-2) - np.roll(p,1,axis=-2))/dx/2\n",
    "bern_dpdy = (np.roll(p,-1,axis=-1) - np.roll(p,1,axis=-1))/dy/2\n",
    "# print(bern_dpdx[0,1])\n",
    "\n",
    "print((dpdx_invrho[0,1]/bern_dpdx[0,1])[3:-3,3:-3])\n",
    "print((dpdy_invrho[0,1]/bern_dpdy[0,1])[3:-3,3:-3])\n",
    "\n",
    "# print(d2udxdy - d2vdxdy)\n",
    "# print(d2vdxdy - check2)\n",
    "\n",
    "# momentum_residual = \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "03d96cfe",
   "metadata": {},
   "outputs": [],
   "source": [
    "pass\n",
    "with open('config.txt','w') as f:\n",
    "    f.write(data.attrs['full_config_str'])\n",
    "\n",
    "with open('physics_config.txt','w') as f:\n",
    "    f.write(data.attrs['physics_config_str'])\n",
    "\n",
    "print(data.attrs['warmup_time'])\n",
    "print(data.attrs['simulation_time'])\n",
    "print(data.attrs['simulation_time'])\n",
    "print(data.stable_time_step)\n",
    "\n",
    "print(data.attrs)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pytorch_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
